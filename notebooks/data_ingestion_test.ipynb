{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c685275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Literal\n",
    "from datetime import datetime\n",
    "\n",
    "class TwitterQuery(BaseModel):\n",
    "    query: str\n",
    "    since: Optional[datetime] = None\n",
    "    until: Optional[datetime] = None\n",
    "    max_results: Optional[int] = Field(default=20, ge=1, le=20)\n",
    "    mode: Literal['Latest', 'Top'] = 'Latest'\n",
    "\n",
    "class SentimentAnswer(BaseModel):\n",
    "    str: str\n",
    "    sentiment: Literal['POSITIVE', 'NEGATIVE', 'NEUTRAL']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f805d",
   "metadata": {},
   "source": [
    "## Crawl data 1h trc -> cache tạm -> hết tiếng đấy thì xuất file cache -> push lên blob -> clean push lên pgsql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e21b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt, timedelta\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfdb1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit import Client\n",
    "import os\n",
    "import dotenv \n",
    "from typing import Literal\n",
    "\n",
    "TW_USERNAME='Akikami_2005'\n",
    "TW_EMAIL='nn20052004@gmail.com'\n",
    "TW_PASSWORD='Hn20052004'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b542cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwikitClient:\n",
    "    def __init__(self, language='en-US'):\n",
    "        self.client = Client(language)\n",
    "        self.is_logged_in = False\n",
    "    \n",
    "    async def login(self, mode=Literal['cookies', 'username']):\n",
    "        \"\"\"Async method to login to Twitter\"\"\"\n",
    "        if mode == 'cookies':\n",
    "            self.client.load_cookies('cookies.json')\n",
    "            print(\"Logged in with cookies\")\n",
    "            self.is_logged_in = True\n",
    "            return\n",
    "        else:\n",
    "            USERNAME = os.getenv('TW_USERNAME')\n",
    "            EMAIL = os.getenv('TW_EMAIL')\n",
    "            PASSWORD = os.getenv('TW_PASSWORD')\n",
    "\n",
    "            await self.client.login(\n",
    "                auth_info_1=USERNAME,\n",
    "                auth_info_2=EMAIL,\n",
    "                password=PASSWORD\n",
    "            )\n",
    "            self.client.save_cookies('cookies.json')\n",
    "            print(\"Logged in with username and password\")\n",
    "            self.is_logged_in = True\n",
    "            return \n",
    "    \n",
    "    async def get_tweets(self, tq: TwitterQuery):\n",
    "        \"\"\"Async method to get tweets\"\"\"\n",
    "        if not self.is_logged_in:\n",
    "            await self.login()\n",
    "            \n",
    "        query_str = f\"{tq.query}\"\n",
    "        if tq.since:\n",
    "            query_str += f\" since:{tq.since.date()}\"\n",
    "        if tq.until:\n",
    "            query_str += f\" until:{tq.until.date()}\"\n",
    "        \n",
    "        tweets = await self.client.search_tweet(query=query_str, product=tq.mode, count=tq.max_results)\n",
    "        return tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14143bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetCache:\n",
    "    def __init__(self):\n",
    "        self.cache = []\n",
    "\n",
    "    def add(self, tweet_list):\n",
    "        \"\"\"Thêm dữ liệu vào cache\"\"\"\n",
    "        self.cache.extend(tweet_list)\n",
    "    \n",
    "    def get_all(self):\n",
    "        \"\"\"Lấy toàn bộ dữ liệu từ cache (không clear)\"\"\"\n",
    "        return self.cache\n",
    "\n",
    "    def save_to_file(self, base_dir, current_hour):\n",
    "        \"\"\"Ghi dữ liệu cache vào file\"\"\"\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "        path = os.path.join(base_dir, f\"raw_{current_hour}_v1.json\")\n",
    "        \n",
    "        # Kiểm tra nếu file đã tồn tại, thì append vào, nếu chưa tạo mới\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r+\", encoding=\"utf-8\") as f:\n",
    "                # Đọc dữ liệu hiện có\n",
    "                existing_data = json.load(f)\n",
    "                existing_data.extend(self.cache)  # Gắn dữ liệu mới vào cuối\n",
    "\n",
    "                # Di chuyển con trỏ về đầu file để ghi đè lại\n",
    "                f.seek(0)\n",
    "                json.dump(existing_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Đã thêm dữ liệu vào: {path}\")\n",
    "        else:\n",
    "            with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(self.cache, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Đã lưu dữ liệu mới tại: {path}\")\n",
    "\n",
    "        # Xóa bộ nhớ cache sau khi lưu vào file\n",
    "        self.cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ebe107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_HASHTAGS = {\n",
    "    'Pope Francis Passing': [\n",
    "        '#RIPPopeFrancis',\n",
    "        '#PopeFrancis',\n",
    "        '#VaticanNews',\n",
    "        '#CatholicChurch'\n",
    "    ],\n",
    "    'US Stocks': [\n",
    "        '#nvda',\n",
    "        '#tsla',\n",
    "        '#aapl',\n",
    "        '#stockmarket',\n",
    "        '#stocks',\n",
    "        '#investing'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716fa4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def crawl_tweet(client, hashtags_dict: dict):\n",
    "    \"\"\"\n",
    "    Crawl tweets for multiple topics and their associated hashtags.\n",
    "\n",
    "    :param client: TwikitClient instance.\n",
    "    :param hashtags_dict: Dictionary where keys are topics and values are lists of hashtags.\n",
    "    :return: Dictionary with topics as keys and crawled tweets as values.\n",
    "    \"\"\"\n",
    "    await client.login(mode='cookies')\n",
    "\n",
    "    tweets_by_topic = dict()\n",
    "    now = dt.now()\n",
    "    fifteen_minutes_ago = now - timedelta(minutes=15)\n",
    "\n",
    "    for topic, hashtags in hashtags_dict.items():\n",
    "        print(f\"Crawling tweets for topic: {topic}\")\n",
    "        topic_tweets = []\n",
    "\n",
    "        # Shuffle hashtags to randomize the order\n",
    "        tags = np.random.permutation(hashtags)\n",
    "\n",
    "        for tag in tags:\n",
    "            tq = TwitterQuery(\n",
    "                query=tag,\n",
    "                mode='Latest',\n",
    "            )\n",
    "            try:\n",
    "                tag_tweets = await client.get_tweets(tq)\n",
    "                topic_tweets.extend([tweet for tweet in tag_tweets])\n",
    "\n",
    "                await asyncio.sleep(15)  # async sleep để tránh lỗi\n",
    "            except Exception as e:\n",
    "                print(f\"Error crawling {tag} for topic {topic}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Store tweets for the current topic\n",
    "        tweets_by_topic[topic] = topic_tweets\n",
    "\n",
    "    return tweets_by_topic\n",
    "\n",
    "def crawl_task(base_dir, hashtags_dict, **context):\n",
    "    \"\"\"\n",
    "    Crawl tweets for multiple topics and save unique tweets to a cache file.\n",
    "\n",
    "    :param base_dir: Directory to save the cache file.\n",
    "    :param hashtags_dict: Dictionary where keys are topics and values are lists of hashtags.\n",
    "    \"\"\"\n",
    "    client = TwikitClient()\n",
    "    tweet_cache = TweetCache()  # Cache để lưu tạm thời\n",
    "\n",
    "    # Tạo event loop mới\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "\n",
    "    # Crawl dữ liệu\n",
    "    tweets_by_topic = loop.run_until_complete(crawl_tweet(client, hashtags_dict))\n",
    "\n",
    "    now = dt.now()\n",
    "    current_hour = now.strftime('%Y%m%d_%H')  # Lấy phần ngày + giờ\n",
    "\n",
    "    seen_texts = set()  # Tập hợp để theo dõi các tweet đã thấy\n",
    "    unique_tweet_list = []  # Danh sách các tweet duy nhất\n",
    "\n",
    "    for topic, tag_tweets in tweets_by_topic.items():\n",
    "        # Signal: In ra số lượng tweet đã crawl được cho từng chủ đề\n",
    "        print(f\"[{now.strftime('%Y-%m-%d %H:%M:%S')}] Crawled {len(tag_tweets)} tweets for topic: {topic}\")\n",
    "\n",
    "        for tweet in tag_tweets:\n",
    "            # Chỉ thêm tweet vào cache nếu nó chưa tồn tại\n",
    "            if tweet.strip() not in seen_texts:\n",
    "                seen_texts.add(tweet.strip())\n",
    "                unique_tweet = {\n",
    "                    'text': tweet.text.strip(),\n",
    "                    'date_created':  tweet.created_at_datetime.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'date_scrape': dt.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'topic': topic  # Lưu chủ đề thay vì hashtag\n",
    "                }\n",
    "                unique_tweet_list.append(unique_tweet)\n",
    "\n",
    "    # Gửi các tweet duy nhất vào cache\n",
    "    tweet_cache.add(unique_tweet_list)\n",
    "\n",
    "    # Lưu cache vào file bằng phương thức save_to_file\n",
    "    tweet_cache.save_to_file(base_dir, current_hour)\n",
    "\n",
    "    print(f\"Data saved for hour {current_hour} with {len(unique_tweet_list)} unique tweets\")\n",
    "    return f\"Data saved for hour {current_hour} with {len(unique_tweet_list)} unique tweets\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "404a4799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in with cookies\n",
      "Crawling tweets for topic: Pope Francis Passing\n",
      "Crawling tweets for topic: US Stocks\n",
      "[2025-05-02 03:34:48] Crawled 61 tweets for topic: Pope Francis Passing\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m      6\u001b[0m BASE_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m././data_cache/raw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#CH test link này\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBASE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhashtags_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTWITTER_HASHTAGS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mĐã crawl xong. File lưu tại: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 73\u001b[0m, in \u001b[0;36mcrawl_task\u001b[1;34m(base_dir, hashtags_dict, **context)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tweet\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen_texts:\n\u001b[0;32m     71\u001b[0m             seen_texts\u001b[38;5;241m.\u001b[39madd(tweet\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m     72\u001b[0m             unique_tweet \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 73\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtweet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m     74\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_created\u001b[39m\u001b[38;5;124m'\u001b[39m:  tweet\u001b[38;5;241m.\u001b[39mcreated_at_datetime\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     75\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_scrape\u001b[39m\u001b[38;5;124m'\u001b[39m: dt\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     76\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m: topic  \u001b[38;5;66;03m# Lưu chủ đề thay vì hashtag\u001b[39;00m\n\u001b[0;32m     77\u001b[0m             }\n\u001b[0;32m     78\u001b[0m             unique_tweet_list\u001b[38;5;241m.\u001b[39mappend(unique_tweet)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Gửi các tweet duy nhất vào cache\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# mặc định là mode='username'\n",
    "client = TwikitClient()\n",
    "# await client.login()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "BASE_DIR = \"././data_cache/raw\" #CH test link này\n",
    "output_path = crawl_task(base_dir=BASE_DIR, hashtags_dict=TWITTER_HASHTAGS)\n",
    "\n",
    "print(f\"Đã crawl xong. File lưu tại: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3fe88",
   "metadata": {},
   "source": [
    "## Lưu vào blob lake raw(đã test lưu raw,ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "class BlobStorage:\n",
    "    \"\"\"\n",
    "    Simple wrapper for Azure Blob Storage operations.\n",
    "    Usage:\n",
    "        blob = BlobStorage()\n",
    "        blob.create_container('raw')\n",
    "        blob.upload_file('raw', 'tweets.json', '/path/to/tweets.json')\n",
    "        data = blob.download_blob('raw', 'tweets.json')\n",
    "    \"\"\"\n",
    "    def __init__(self, conn_str: str):\n",
    "        self.service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
    "\n",
    "    def create_container(self, container_name: str):\n",
    "        \"\"\"\n",
    "        Create a container if it does not exist.\n",
    "        \"\"\"\n",
    "        container = self.service_client.get_container_client(container_name)\n",
    "        try:\n",
    "            container.create_container()\n",
    "        except Exception:\n",
    "            # Container may already exist\n",
    "            pass\n",
    "        return container\n",
    "\n",
    "    def upload_file(self, container_name: str, blob_name: str, file_path: str, overwrite: bool = True):\n",
    "        \"\"\"\n",
    "        Upload a local file to a blob with input validation and enhanced error handling.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Kiểm tra nếu file cục bộ tồn tại\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"Local file '{file_path}' does not exist.\")\n",
    "\n",
    "            # Lấy container client\n",
    "            container = self.service_client.get_container_client(container_name)\n",
    "\n",
    "            # Kiểm tra nếu container không tồn tại, tạo mới\n",
    "            if not container.exists():\n",
    "                container.create_container()\n",
    "                print(f\"Container '{container_name}' created.\")\n",
    "\n",
    "            # Lấy blob client\n",
    "            blob = container.get_blob_client(blob_name)\n",
    "\n",
    "            # Mở file và tải lên blob\n",
    "            with open(file_path, 'rb') as data:\n",
    "                blob.upload_blob(data, overwrite=overwrite)\n",
    "            print(f\"File '{file_path}' uploaded to blob '{blob_name}' in container '{container_name}'.\")\n",
    "\n",
    "            # Trả về URL của blob\n",
    "            return blob.url\n",
    "\n",
    "        except FileNotFoundError as fnf_error:\n",
    "            print(fnf_error)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading file '{file_path}' to blob '{blob_name}': {e}\")\n",
    "            raise\n",
    "    \n",
    "    def upload_blob(self,container_name, blob_name, data, overwrite=False):\n",
    "        \"\"\"\n",
    "        Upload data to Azure Blob Storage. It ensures the blob name is valid.\n",
    "\n",
    "        Parameters:\n",
    "        - blob_name: The name of the blob (including folder structure).\n",
    "        - data: The data to be uploaded (bytes).\n",
    "        - overwrite: Boolean flag to overwrite if the blob already exists.\n",
    "        \"\"\"\n",
    "        # Remove any trailing slashes from blob name to avoid invalid resource name\n",
    "        blob_name = blob_name.rstrip('/')\n",
    "        \n",
    "        # Check if the blob name is valid\n",
    "        if any(c in blob_name for c in ['\\\\', ':', '*', '?', '\"', '<', '>', '|']):\n",
    "            raise ValueError(f\"Blob name contains invalid characters: {blob_name}\")\n",
    "        \n",
    "        container = self.service_client.get_container_client(container_name)\n",
    "        blob_client = container.get_blob_client(blob_name)\n",
    "\n",
    "        try:\n",
    "            # Upload the data\n",
    "            blob_client.upload_blob(data, overwrite=overwrite)\n",
    "            print(f\"Blob '{blob_name}' uploaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading blob '{blob_name}': {e}\")\n",
    "\n",
    "    def upload_bytes(self, container: str, blob: str, data: bytes | str, *, overwrite: bool = True, blob_type: str = \"BlockBlob\") -> str:\n",
    "        \"\"\"Upload bytes or string to blob.\"\"\"\n",
    "        blob_c = self.client.get_blob_client(container, blob)\n",
    "        \n",
    "        # Chuyển đổi dữ liệu nếu là chuỗi\n",
    "        if isinstance(data, str):\n",
    "            data = data.encode('utf-8')  # Mã hóa chuỗi thành byte\n",
    "        \n",
    "        # Tải lên dữ liệu (bytes)\n",
    "        blob_c.upload_blob(data, overwrite=overwrite, blob_type=blob_type)\n",
    "        \n",
    "        # Trả về URL của blob vừa tải lên\n",
    "        return blob_c.url\n",
    "\n",
    "    def create_folder_structure(self, container_name, folder_prefix):\n",
    "        \"\"\"\n",
    "        Ensure folder structure is created in the container by uploading dummy files.\n",
    "\n",
    "        Parameters:\n",
    "        - folder_prefix: The folder structure to create.\n",
    "        \"\"\"\n",
    "        # Ensure the folder structure exists by uploading a dummy file\n",
    "        dummy_data = b\"\"  # Empty data for a dummy file\n",
    "        \n",
    "        # Upload an init file to represent the folder structure\n",
    "        self.upload_blob(container_name, f\"{folder_prefix}/.init\", dummy_data, overwrite=True)\n",
    "\n",
    "\n",
    "    def download_blob(self, container_name: str, blob_name: str) -> bytes:\n",
    "        \"\"\"\n",
    "        Download blob content and return as bytes.\n",
    "        \"\"\"\n",
    "        blob = self.service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "        downloader = blob.download_blob()\n",
    "        return downloader.readall()\n",
    "\n",
    "    def download_file(self, container: str, blob: str, path: str) -> str:\n",
    "        \"\"\"Download blob and write directly to file (streaming).\"\"\"\n",
    "        blob_client = self.service_client.get_blob_client(container, blob)\n",
    "        with open(path, \"wb\") as f:\n",
    "            blob_client.download_blob().readinto(f)\n",
    "        return path\n",
    "    \n",
    "    def read_json_from_container(self, container: str, file_path: str):\n",
    "        \"\"\"Read a JSON file directly from container into RAM without saving to local.\"\"\"\n",
    "        # 1. Lấy blob client từ container và đường dẫn file\n",
    "        blob_client = self.service_client.get_blob_client(container=container, blob=file_path)\n",
    "        # 2. Download toàn bộ nội dung file vào RAM\n",
    "        blob_data = blob_client.download_blob().readall()        \n",
    "        # 3. Parse JSON từ bytes\n",
    "        data = json.loads(blob_data.decode('utf-8'))  # phải decode từ bytes -> str trước\n",
    "\n",
    "        return data\n",
    "\n",
    "    def list_blobs_by_path(self, container: str, path: str = \"\") -> list[str]:\n",
    "        \"\"\"\n",
    "        List blobs or pseudo-folders in a given path within a container.\n",
    "        If path is empty, list top-level blobs/folders.\n",
    "        \"\"\"\n",
    "        container_client = self.service_client.get_container_client(container)\n",
    "        path = path.strip('/')\n",
    "        if path:\n",
    "            path += \"/\"\n",
    "\n",
    "        # Dùng delimiter để liệt kê như thư mục\n",
    "        blob_list = container_client.walk_blobs(name_starts_with=path, delimiter='/')\n",
    "        return [blob.name for blob in blob_list]\n",
    "\n",
    "    def delete_blob(self, container_name: str, blob_name: str):\n",
    "        \"\"\"\n",
    "        Delete a blob from a container.\n",
    "        \"\"\"\n",
    "        blob = self.service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "        blob.delete_blob()\n",
    "\n",
    "    def delete_container(self, container_name: str):\n",
    "        \"\"\"\n",
    "        Delete a container and all its blobs.\n",
    "        \"\"\"\n",
    "        container = self.service_client.get_container_client(container_name)\n",
    "        container.delete_container()\n",
    "\n",
    "    def close(self):\n",
    "        self.service_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8b81df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "conn_str = os.getenv('LAKE_STORAGE_CONN_STR')\n",
    "if not conn_str:\n",
    "    raise EnvironmentError(\"LAKE_STORAGE_CONN_STR not found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c26c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../data_cache/raw/raw.json' uploaded to blob 'raw/raw.json' in container 'data'.\n",
      "File uploaded successfully. Blob URL: https://testlakehouse.blob.core.windows.net/data/raw/raw.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    blob_storage = BlobStorage(conn_str)\n",
    "    container_name = \"data\"  \n",
    "    blob_name = \"raw/raw.json\"\n",
    "    file_path = \"../data_cache/raw/raw.json\"\n",
    "\n",
    "    # Kiểm tra file cục bộ\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Local file '{file_path}' does not exist.\")\n",
    "\n",
    "    # Kiểm tra nếu file rỗng\n",
    "    if os.stat(file_path).st_size == 0:\n",
    "        raise ValueError(f\"File '{file_path}' is empty and will not be uploaded.\")\n",
    "\n",
    "    # Tải file lên\n",
    "    blob_url = blob_storage.upload_file(\n",
    "        container_name=container_name,\n",
    "        blob_name=blob_name,\n",
    "        file_path=file_path,\n",
    "        overwrite=True\n",
    "    )\n",
    "    print(f\"File uploaded successfully. Blob URL: {blob_url}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0d90c",
   "metadata": {},
   "source": [
    "## Pull code (.dvc sẽ làm sau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "18c03ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from typing import List, Dict, Optional\n",
    "from psycopg2 import extras\n",
    "\n",
    "\n",
    "class Database:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the database connection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                host=kwargs.get(\"host\"),\n",
    "                port=kwargs.get(\"port\"),\n",
    "                dbname=kwargs.get(\"dbname\"),\n",
    "                user=kwargs.get(\"user\"),\n",
    "                password=kwargs.get(\"password\"),\n",
    "                sslmode=kwargs.get(\"sslmode\", \"prefer\")\n",
    "            )\n",
    "            self.cursor = self.connection.cursor()\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to the database: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_table(self, table_name: str, columns: Dict[str, str]):\n",
    "        \"\"\"\n",
    "        Create a table with the specified columns.\n",
    "\n",
    "        :param table_name: Name of the table.\n",
    "        :param columns: Dictionary of column names and their data types.\n",
    "        \"\"\"\n",
    "        column_defs = [\n",
    "            f\"{col} {dtype}\" if \"FOREIGN KEY\" not in dtype else f\"{col} {dtype.split(', FOREIGN KEY ')[0]} {dtype.split(', FOREIGN KEY ')[1]}\"\n",
    "            for col, dtype in columns.items()\n",
    "        ]\n",
    "        query = sql.SQL(\"CREATE TABLE IF NOT EXISTS {} ({})\").format(\n",
    "            sql.Identifier(table_name),\n",
    "            sql.SQL(\", \").join(map(sql.SQL, column_defs))\n",
    "        )\n",
    "        self._execute_query(query, f\"Error creating table {table_name}\")\n",
    "\n",
    "    def insert(self, table_name: str, data: Dict[str, any]):\n",
    "        \"\"\"\n",
    "        Insert a record into the specified table.\n",
    "\n",
    "        :param table_name: Name of the table.\n",
    "        :param data: Dictionary of column names and their values.\n",
    "        \"\"\"\n",
    "        columns = data.keys()\n",
    "        values = tuple(data.values())\n",
    "        query = sql.SQL(\"INSERT INTO {} ({}) VALUES ({})\").format(\n",
    "            sql.Identifier(table_name),\n",
    "            sql.SQL(\", \").join(map(sql.Identifier, columns)),\n",
    "            sql.SQL(\", \").join(sql.Placeholder() * len(values))\n",
    "        )\n",
    "        self._execute_query(query, f\"Error inserting data into {table_name}\", values)\n",
    "\n",
    "    def get_columns(self, table_name: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get the list of columns in the specified table.\n",
    "\n",
    "        :param table_name: Name of the table.\n",
    "        :return: List of column names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Chuyển tên bảng sang chữ thường\n",
    "\n",
    "            query = sql.SQL(\n",
    "                \"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = {}\"\n",
    "            ).format(sql.Literal(table_name))\n",
    "            self.cursor.execute(query)  # Sử dụng sql.Literal để tránh lỗi định dạng\n",
    "            return self.cursor.fetchall()\n",
    "        except Exception as e:\n",
    "            self.connection.rollback()  # Rollback giao dịch nếu có lỗi\n",
    "            print(f\"Error fetching columns for table '{table_name}': {e}\")\n",
    "            return []\n",
    "\n",
    "    def read(self, table_name: str, columns: Optional[List[str]] = None, conditions: Optional[Dict[str, any]] = None, limit: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Read data from the specified table.\n",
    "\n",
    "        :param table_name: Name of the table.\n",
    "        :param columns: List of columns to fetch (default: all columns).\n",
    "        :param conditions: Dictionary of conditions for the WHERE clause.\n",
    "        :param limit: Maximum number of rows to fetch.\n",
    "        :return: List of rows.\n",
    "        \"\"\"\n",
    "        columns_sql = sql.SQL(\", \").join(map(sql.Identifier, columns)) if columns else sql.SQL(\"*\")\n",
    "        query = sql.SQL(\"SELECT {} FROM {}\").format(columns_sql, sql.Identifier(table_name))\n",
    "        if conditions:\n",
    "            where_clause = sql.SQL(\" WHERE \") + sql.SQL(\" AND \").join(\n",
    "                sql.Composed([sql.Identifier(k), sql.SQL(\" = \"), sql.Placeholder(k)]) for k in conditions.keys()\n",
    "            )\n",
    "            query += where_clause\n",
    "        if limit:\n",
    "            query += sql.SQL(\" LIMIT {}\").format(sql.Literal(limit))\n",
    "        return self._fetch_query(query, f\"Error reading data from {table_name}\", conditions)\n",
    "\n",
    "    def update(self, table_name: str, updates: Dict[str, any], conditions: Dict[str, any]):\n",
    "        \"\"\"\n",
    "        Update records in the specified table.\n",
    "\n",
    "        :param table_name: Name of the table.\n",
    "        :param updates: Dictionary of columns to update and their new values.\n",
    "        :param conditions: Dictionary of conditions for the WHERE clause.\n",
    "        \"\"\"\n",
    "        set_clause = sql.SQL(\", \").join(\n",
    "            sql.Composed([sql.Identifier(k), sql.SQL(\" = \"), sql.Placeholder(f\"set_{k}\")]) for k in updates.keys()\n",
    "        )\n",
    "        where_clause = sql.SQL(\" AND \").join(\n",
    "            sql.Composed([sql.Identifier(k), sql.SQL(\" = \"), sql.Placeholder(f\"where_{k}\")]) for k in conditions.keys()\n",
    "        )\n",
    "        query = sql.SQL(\"UPDATE {} SET {} WHERE {}\").format(\n",
    "            sql.Identifier(table_name), set_clause, where_clause\n",
    "        )\n",
    "        params = {f\"set_{k}\": v for k, v in updates.items()}\n",
    "        params.update({f\"where_{k}\": v for k, v in conditions.items()})\n",
    "        self._execute_query(query, f\"Error updating data in {table_name}\", params)\n",
    "\n",
    "    def delete(self, table_name: str, conditions: Dict[str, any]):\n",
    "        \"\"\"\n",
    "        Delete records from the specified table.\n",
    "\n",
    "        :param table_name: Name of the table.\n",
    "        :param conditions: Dictionary of conditions for the WHERE clause.\n",
    "        \"\"\"\n",
    "        where_clause = sql.SQL(\" AND \").join(\n",
    "            sql.Composed([sql.Identifier(k), sql.SQL(\" = \"), sql.Placeholder(k)]) for k in conditions.keys()\n",
    "        )\n",
    "        query = sql.SQL(\"DELETE FROM {} WHERE {}\").format(sql.Identifier(table_name), where_clause)\n",
    "        self._execute_query(query, f\"Error deleting data from {table_name}\", conditions)\n",
    "\n",
    "    def _execute_query(self, query, error_message, params=None):\n",
    "        \"\"\"\n",
    "        Execute a query and handle errors.\n",
    "\n",
    "        :param query: SQL query to execute.\n",
    "        :param error_message: Error message to display if the query fails.\n",
    "        :param params: Parameters for the query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.cursor.execute(query, params)\n",
    "            self.connection.commit()\n",
    "        except Exception as e:\n",
    "            self.connection.rollback()\n",
    "            print(f\"{error_message}: {e}\")\n",
    "\n",
    "    def _fetch_query(self, query, error_message, params=None):\n",
    "        \"\"\"\n",
    "        Execute a query and fetch results.\n",
    "\n",
    "        :param query: SQL query to execute.\n",
    "        :param error_message: Error message to display if the query fails.\n",
    "        :param params: Parameters for the query.\n",
    "        :return: Query results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.cursor.execute(query, params)\n",
    "            return self.cursor.fetchall()\n",
    "        except Exception as e:\n",
    "            self.connection.rollback()\n",
    "            print(f\"{error_message}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close the database connection.\n",
    "        \"\"\"\n",
    "        self.cursor.close()\n",
    "        self.connection.close()\n",
    "\n",
    "    def batch_insert(self, table_name: str, tweets_data: list, column_mapping: dict):\n",
    "            \"\"\"\n",
    "            Insert multiple records into a table based on column mapping.\n",
    "\n",
    "            :param table_name: Name of the table.\n",
    "            :param tweets_data: List of dictionaries containing tweet data.\n",
    "            :param column_mapping: Dictionary mapping input fields to database columns.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # Map input fields to database columns\n",
    "                columns = list(column_mapping.values())\n",
    "\n",
    "                # Prepare data for insertion\n",
    "                values = [\n",
    "                    tuple(tweet[field] for field in column_mapping.keys())\n",
    "                    for tweet in tweets_data\n",
    "                ]\n",
    "\n",
    "                # Use psycopg2.extras.execute_values for batch insert\n",
    "                insert_query = sql.SQL(\"INSERT INTO {} ({}) VALUES %s\").format(\n",
    "                    sql.Identifier(table_name),\n",
    "                    sql.SQL(\", \").join(map(sql.Identifier, columns))\n",
    "                )\n",
    "                extras.execute_values(self.cursor, insert_query, values)\n",
    "                self.connection.commit()\n",
    "                print(f\"Successfully inserted {len(tweets_data)} records into {table_name}.\")\n",
    "            except Exception as e:\n",
    "                self.connection.rollback()\n",
    "                print(f\"Error inserting batch data into {table_name}: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368c78b",
   "metadata": {},
   "source": [
    "## Pull raw ra clean và pull lên pqsql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "from twikit import Tweet\n",
    "import time\n",
    "import json \n",
    "import pandas as pd\n",
    "import string\n",
    "from typing import List, Optional\n",
    "\n",
    "LLM_API_KEY =''\n",
    "\n",
    "def clean_data(raw: List[str] = None):\n",
    "    if not raw:\n",
    "        return \n",
    "    #text = [t.text for t in raw]\n",
    "\n",
    "    # Remove all emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U00002600-\\U000026FF\"  # Misc symbols\n",
    "        \"\\U000025A0-\\U000025FF\"  # Geometric Shapes\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "\n",
    "    cleaned_tweets = [re.sub(emoji_pattern, '', t) for t in raw]\n",
    "\n",
    "    # Remove the hashtags that's in crawling list\n",
    "    for tag in TWITTER_HASHTAGS:\n",
    "        cleaned_tweets = [t.replace(tag, '') for t in cleaned_tweets]\n",
    "\n",
    "    # Remove newlines\n",
    "    cleaned_tweets = [t.replace('\\n', ' ').replace('\\r', ' ') for t in cleaned_tweets]\n",
    "\n",
    "    # Remove links\n",
    "    cleaned_tweets = [re.sub(r'https://t.co/\\w+', '', t) for t in cleaned_tweets]\n",
    "\n",
    "    # Remove trailing whitespaces\n",
    "    cleaned_tweets = [t.strip() for t in cleaned_tweets]\n",
    "\n",
    "    # Remove empty\n",
    "\n",
    "    cleaned_tweets = [t for t in cleaned_tweets if t != '']\n",
    "\n",
    "    return cleaned_tweets\n",
    "\n",
    "def clean_data_v2(raw: Optional[List[str]] = None, hashtags: Optional[List[str]] = None) -> List[str]:\n",
    "    if not raw:\n",
    "        return []\n",
    "    \n",
    "    if hashtags is None:\n",
    "        hashtags = TWITTER_HASHTAGS\n",
    "\n",
    "    # Regex patterns\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"\\U00002700-\\U000027BF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U00002600-\\U000026FF\"\n",
    "        \"\\U000025A0-\\U000025FF\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    url_pattern = re.compile(r'https?://\\S+')\n",
    "\n",
    "    # Punctuation to remove\n",
    "    punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    cleaned = []\n",
    "    for text in raw:\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove emojis\n",
    "        text = emoji_pattern.sub('', text)\n",
    "\n",
    "        # Remove hashtags\n",
    "        for tag in hashtags:\n",
    "            text = text.replace(tag.lower(), '')  # lowercase để chắc chắn match\n",
    "\n",
    "        # Remove links\n",
    "        text = url_pattern.sub('', text)\n",
    "\n",
    "        # Remove newlines, carriage returns\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(punctuation_table)\n",
    "\n",
    "        # Remove leading/trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        if text:\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f82921bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'we are reliable to offer you a grade in your nursing essays chemistry finance physics biology physics chemistry algebra calculus essay pay assignment due 100daysofcode womenwhocode javascript serverless asu iothit our bio for more info', 'date_scraped': '2025-04-28', 'tag': '#Python'}, {'text': 'upgrading django run python wa test to catch deprecation warnings early and ensure a smoother upgrade with fewer surprises devcommunity', 'date_scraped': '2025-04-28', 'tag': '#Python'}, {'text': 'extend vs append', 'date_scraped': '2025-04-28', 'tag': '#Python'}, {'text': 'day 23 of 100daysofcode the day counting was wrong when i compared it with the tracker i have on my github repo solved couple of problems based on writing unit tests unit tests were written for problems i previously solved need to write more psuedo code cs50', 'date_scraped': '2025-04-28', 'tag': '#Python'}, {'text': 'prometheus masterclass infra monitoring amp alerting gt 100daysofcode programming c java c react javascript dataanalysis nodejs algorithms datavisualization hibernate datastructures udemy free coupons', 'date_scraped': '2025-04-28', 'tag': '#Python'}, {'text': 'day 15100 of python learning today i have learned of how to use functions in python codebreakers coder codinglife codinglife codingdaily learning series codeisfun code', 'date_scraped': '2025-04-28', 'tag': '#Python'}, {'text': 'day 35 gfg160  found the kth missing number solved with on time complexity scanned sorted array linearly tracked missing numbers efficiently maintained 100 accuracy key insight compare expected vs actual values geeksforgeeks geekstreak2025 dsa', 'date_scraped': '2025-04-28', 'tag': '#Python'}, {'text': 'slidespeak is hiring full stack ai software engineer 100 remote wfh remotejobs ready for your next career move details amp application', 'date_scraped': '2025-04-28', 'tag': '#Python'}, {'text': 'what if all that ecls we’ve been doing… isn’t helping ️ analysis of the eclsshock trial shows minimal impact across the board provocative findings here cardiocriticalcare jaccint thieleholger kpkresoja', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'just posted a quora answer on whether ai will end coding spoiler it’s a gamechanger not a replacement shared how grok 3 slashed my ml preprocessing from hours to minutes amp dropped a parallel processing template on github check it out coding', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'automation is constantly advancing with the integration of ai and machine learning enabling faster and more efficient processes increased accuracy and improved decisionmaking capabilities automation efficiency', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'finished python warmcold guessing game started functions module and practiced a lot of exercises reviewed ac thevenin and norton theorems and ac power analysis  resources   math ml', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'the future is autonomous ai agents powered by llms  realworld tools smartdataincltd techtrends', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'lithos an operating system for efficient machine learning on gpus cuda operatingsystem os ml', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'hojjatbakhshi01 hyperboliclabs when compute barriers fall hyperbolic fuels innovation unlocking creativity breakthroughs and true techfreedom kaito', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'algorithms cheat sheet ml algorithms supervisedlearning unsupervisedlearning reinforcementlearning semisupervisedlearning', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'prediction w 42725 by sabah atieh djia nasdaq eurusd usdjpy wti brent us10yr bitcoin forex commodities treasuries boj boe ecb fomc datascience', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': '10 simple steps to optimize production with by antgrasso dataanalysis datascience artificialintelligence deeplearning cc meisshaily hanaelsayyed pierrepinna', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'darpa calls for ai proposals to accelerate math research  the register', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'lessons learned from 2 rfid veterans  pharmacy practice news', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'exciting advancements in ai technology from breakthroughs in natural language processing to cuttingedge machine learning algorithms the future of ai is bright technology nlproc innovation poweredbyworld3', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'epic tech ai platform a comprehensive and agentic media creation platform that leverages advanced ai technologies to generate and enhance content in various formats myaitutor built in less then 5 minutes tutormachinebuilders innovation techcommunity', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'embracing the future with and  at buildera were harnessing these technologies to revolutionize software solutions share your thoughts on how ai is reshaping industries lets push the tech boundaries together techtalk innovation', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'despite the boom the majority of companies struggle to achieve profitable outcomes according to forbes businessstrategy fintech tech techpr businessstrategy startups', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'taking some time off from blockchain gods this weekend to create a dataset for an ml app data ios', 'date_scraped': '2025-04-28', 'tag': '#MachineLearning'}, {'text': 'the gaming world didn’t crash it evolved in 2025 it’s ai vs consolidation vs survival are you adapting fast enough gamingindustry', 'date_scraped': '2025-04-28', 'tag': '#AI'}, {'text': 'bittensors decentralized ai network is redefining how artificial intelligence will develop by incentivizing contributions through tao tokens bittensor is ensuring that the best ai models thrive driving innovation across industries bittensor blockchain decentralizedai', 'date_scraped': '2025-04-28', 'tag': '#AI'}, {'text': 'bsiness 101 exploring opportunities in blockchain ai amp sustainable technologies sponsored by tsabitcoinshop each fri and sat 1347 marine drive west vancouver scan barcode to register  blockchain sustainabletech business101', 'date_scraped': '2025-04-28', 'tag': '#AI'}, {'text': 'aiwhitebridge token2049 tarpaulius tomasmartunas excited to see tarpaulius and tomasmartunas leading the ai \\u200b\\u200band web3 discussions in dubai their exploration of innovative solutions in identity and trust will pave the way for the future of web3 chaingpt chaingptpad web3 cgpt', 'date_scraped': '2025-04-28', 'tag': '#AI'}, {'text': 'saudi arabia to overhaul statistical systems with ai and big data to drive vision 2030 decisions saudidata vision2030', 'date_scraped': '2025-04-28', 'tag': '#AI'}, {'text': 'a new wallet app swissknifelike is coming stay tuned sniperbot tradingcrypto', 'date_scraped': '2025-04-28', 'tag': '#AI'}, {'text': 'bittensors decentralized ai network is redefining how artificial intelligence will develop by incentivizing contributions through tao tokens bittensor is ensuring that the best ai models thrive driving innovation across industries bittensor blockchain decentralizedai', 'date_scraped': '2025-04-28', 'tag': '#AI'}, {'text': 'sobre el gran desafío de humanidad y la ia opinión de un experto e iniciadores poder desigualdad desempleo democracia', 'date_scraped': '2025-04-28', 'tag': '#AI'}, {'text': 'can unlock the prognostic power of tertiary lymphoid structures tls using just hampe staining don’t miss ahmad tarhini md phd atarhinimdphd present at aacr25 on the potential of aidriven approaches to standardize tls assessment to improve stratification within ajcc', 'date_scraped': '2025-04-28', 'tag': '#AI'}, {'text': 'the end drop me a follow for more such content on programming and startups chatgpt vibecoding claude cursor', 'date_scraped': '2025-04-28', 'tag': '#AI'}]\n"
     ]
    }
   ],
   "source": [
    "blob_storage = BlobStorage(conn_str)\n",
    "data = blob_storage.read_json_from_container(container=\"data\", file_path=\"raw/raw.json\")\n",
    "# Trích xuất trường 'text' và làm sạch dữ liệu\n",
    "texts = [item['text'] for item in data]\n",
    "cleaned_texts = clean_data_v2(texts)\n",
    "\n",
    "# Lồng dữ liệu đã làm sạch vào lại data (cập nhật trường 'text')\n",
    "for i, item in enumerate(data):\n",
    "    item['text'] = cleaned_texts[i]\n",
    "\n",
    "# In lại dữ liệu đã được cập nhật\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f245bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "user = os.getenv('PG_USER')\n",
    "host = os.getenv('PG_HOST')\n",
    "port = os.getenv('PG_PORT')\n",
    "password = os.getenv('PG_PASSWORD')\n",
    "dbname = os.getenv('PG_DB1')\n",
    "sslmode = os.getenv('PG_SSLMODE')\n",
    "\n",
    "# Initialize database connection\n",
    "db = Database(\n",
    "    host=host,\n",
    "    port=port,\n",
    "    dbname=dbname,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    sslmode=sslmode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "10eb5217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inserted 35 records into TWEET_STAGING.\n"
     ]
    }
   ],
   "source": [
    "column_mapping = {\n",
    "    \"text\": \"content\",\n",
    "    \"date_scraped\": \"crawl_at\",\n",
    "    \"tag\": \"topic\"\n",
    "}\n",
    "\n",
    "db.batch_insert(table_name=\"TWEET_STAGING\", tweets_data=data, column_mapping=column_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29d613",
   "metadata": {},
   "source": [
    "# Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "import psycopg2\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.functions import HttpRequest, HttpResponse\n",
    "\n",
    "def parse_request(req: HttpRequest) -> str:\n",
    "    \"\"\"Parse the HTTP request to extract the blob name.\"\"\"\n",
    "    try:\n",
    "        req_body = req.get_json()\n",
    "        blob_name = req_body.get('blob_name')\n",
    "        if not blob_name:\n",
    "            raise ValueError(\"Missing 'blob_name' in request payload.\")\n",
    "        return blob_name\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing request: {e}\")\n",
    "\n",
    "def get_blob_data(conn_str: str, container_name: str, blob_name: str) -> list:\n",
    "    \"\"\"Download and parse blob content from Azure Blob Storage.\"\"\"\n",
    "    try:\n",
    "        blob_storage = BlobStorage(conn_str)\n",
    "        blob_data = blob_storage.download_blob(container_name, blob_name)\n",
    "        return json.loads(blob_data)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error downloading blob: {e}\")\n",
    "\n",
    "def insert_data_to_postgres(data: list, db_config: dict):\n",
    "    \"\"\"Insert data into PostgreSQL using a stored procedure.\"\"\"\n",
    "    try:\n",
    "        pg_conn = psycopg2.connect(**db_config)\n",
    "        cursor = pg_conn.cursor()\n",
    "        for record in data:\n",
    "            cursor.execute(\"CALL insert_into_staging(%s, %s, %s, %s)\", (\n",
    "                record['text'],\n",
    "                record['tag'],\n",
    "                record['date_scraped'],\n",
    "                record['timestamp']\n",
    "            ))\n",
    "        pg_conn.commit()\n",
    "        cursor.close()\n",
    "        pg_conn.close()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error inserting data into PostgreSQL: {e}\")\n",
    "\n",
    "def main(req: HttpRequest) -> HttpResponse:\n",
    "    logging.info('Azure Function triggered to process Blob and insert into PostgreSQL.')\n",
    "\n",
    "    try:\n",
    "        # Parse the request\n",
    "        blob_name = parse_request(req)\n",
    "        #Init blobblob\n",
    "        blob_storage = BlobStorage(conn_str)\n",
    "        # Azure Blob Storage connection\n",
    "        conn_str = os.getenv('LAKE_STORAGE_CONN_STR')\n",
    "        container_name = 'data/raw'\n",
    "\n",
    "        # Download blob content\n",
    "        data = get_blob_data(conn_str, container_name, blob_name)\n",
    "\n",
    "        # PostgreSQL connection configuration\n",
    "        db_config = {\n",
    "            'dbname': os.getenv('PG_DB1'),\n",
    "            'user': os.getenv('PG_USER'),\n",
    "            'password': os.getenv('PG_PASSWORD'),\n",
    "            'host': os.getenv('PG_HOST'),\n",
    "            'port': os.getenv('PG_PORT'),\n",
    "            'sslmode': os.getenv('PG_SSLMODE')\n",
    "        }\n",
    "\n",
    "        # Insert data into PostgreSQL\n",
    "        insert_data_to_postgres(data, db_config)\n",
    "\n",
    "        logging.info(f\"Successfully processed blob: {blob_name} and inserted data into PostgreSQL.\")\n",
    "        return HttpResponse(f\"Successfully processed blob: {blob_name}.\", status_code=200)\n",
    "\n",
    "    except ValueError as ve:\n",
    "        logging.error(f\"Request error: {ve}\")\n",
    "        return HttpResponse(str(ve), status_code=400)\n",
    "    except RuntimeError as re:\n",
    "        logging.error(f\"Processing error: {re}\")\n",
    "        return HttpResponse(str(re), status_code=500)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "        return HttpResponse(f\"Error processing blob: {e}\", status_code=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58180774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a57540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import requests\n",
    "from your_module import crawl_task, BlobStorage\n",
    "\n",
    "# Default arguments for the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(\n",
    "    'tweet_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Crawl tweets, upload to blob, and trigger staging DB via Azure Function',\n",
    "    schedule_interval='@hourly',  # Run every hour\n",
    "    start_date=datetime(2025, 4, 28),\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    # Task 1: Crawl tweets and save to a local file\n",
    "    def crawl_tweets(**kwargs):\n",
    "        base_dir = \"./data_cache/raw\"\n",
    "        hashtags = [\"#AI\", \"#MachineLearning\", \"#Python\"]\n",
    "        output_path = crawl_task(base_dir=base_dir, hashtags=hashtags)\n",
    "        return output_path\n",
    "\n",
    "    # Task 2: Upload the file to Azure Blob Storage\n",
    "    def upload_to_blob(**kwargs):\n",
    "        ti = kwargs['ti']\n",
    "        file_path = ti.xcom_pull(task_ids='crawl_tweets')  # Get file path from previous task\n",
    "        conn_str = os.getenv('LAKE_STORAGE_CONN_STR')\n",
    "        blob_storage = BlobStorage(conn_str)\n",
    "        blob_storage.upload_file('raw', os.path.basename(file_path), file_path)\n",
    "        print(f\"Uploaded {file_path} to Blob Storage\")\n",
    "        return os.path.basename(file_path)  # Return the blob name for the next task\n",
    "\n",
    "    # Task 3: Call Azure Function to trigger the staging database\n",
    "    def call_azure_function(**kwargs):\n",
    "        ti = kwargs['ti']\n",
    "        blob_name = ti.xcom_pull(task_ids='upload_to_blob')  # Get blob name from previous task\n",
    "\n",
    "        # Azure Function endpoint\n",
    "        azure_function_url = \"https://your-azure-function-url/trigger-staging\"\n",
    "        payload = {\"blob_name\": blob_name}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "        response = requests.post(azure_function_url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Successfully triggered Azure Function for blob: {blob_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to trigger Azure Function. Status code: {response.status_code}, Response: {response.text}\")\n",
    "\n",
    "    # Define tasks\n",
    "    crawl_tweets_task = PythonOperator(\n",
    "        task_id='crawl_tweets',\n",
    "        python_callable=crawl_tweets,\n",
    "        provide_context=True,\n",
    "    )\n",
    "\n",
    "    upload_to_blob_task = PythonOperator(\n",
    "        task_id='upload_to_blob',\n",
    "        python_callable=upload_to_blob,\n",
    "        provide_context=True,\n",
    "    )\n",
    "\n",
    "    call_azure_function_task = PythonOperator(\n",
    "        task_id='call_azure_function',\n",
    "        python_callable=call_azure_function,\n",
    "        provide_context=True,\n",
    "    )\n",
    "\n",
    "    # Set task dependencies\n",
    "    crawl_tweets_task >> upload_to_blob_task >> call_azure_function_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ccf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azure-functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
