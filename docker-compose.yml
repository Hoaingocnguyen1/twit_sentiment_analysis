# # --- common environment for Airflow services ---
x-airflow-common-env: &airflow-common-env
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql://hoaingocnguyen:Hn20052004!@twitwarehouse.postgres.database.azure.com:5432/postgres?sslmode=require
  AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
  AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
  AIRFLOW__CORE__PLUGINS_FOLDER: /opt/airflow/plugins
  AIRFLOW__CORE__BASE_LOG_FOLDER: /opt/airflow/logs
  AIRFLOW__WEBSERVER__SECRET_KEY: "my_ultra_secret_key_123456"
  PYTHONPATH: /opt/airflow

# --- common service definition for Airflow ---
x-airflow-common: &airflow-common
  build:
    context: ./airflow
    dockerfile: Dockerfile
  environment:
    <<: *airflow-common-env
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/plugins:/opt/airflow/plugins
    - ./src:/opt/airflow/src
    - ./config:/opt/airflow/config
    - ./airflow/requirements.txt:/requirements.txt
    - ./config/cookies.json:/opt/airflow/cookies.json
    - ./.env:/opt/airflow/.env

# --- logging configuration anchor ---
x-log-config: &log-config
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "1"

  # Airflow Webserver
  # airflow-webserver:
  #   <<: *airflow-common
  #   hostname: airflow-webserver
  #   ports:
  #     - "8080:8080"
  #   expose:
  #     - "8080"
  #   logging:
  #     <<: *log-config
  #   command: >
  #     bash -c "\
  #       airflow db init && \
  #       airflow users create --username admin --password admin_password --firstname FirstName --lastname LastName --email nn20052004@gmail.com --role Admin && \
  #       exec airflow webserver"
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 5

  # Airflow Scheduler
  # airflow-scheduler:
  #   <<: *airflow-common
  #   hostname: airflow-scheduler
  #   logging:
  #     <<: *log-config
  #   command: >
  #     bash -c "exec airflow scheduler"
  #   healthcheck:
  #     test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob"]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 5

services:
  mlflow-server:
    build:
      context: ./mlflow
      dockerfile: Dockerfile
    env_file:
      - .env
    environment:
      - AZURE_STORAGE_CONNECTION_STRING=${LAKE_STORAGE_CONN_STR}
      - MLPGSQL_CONN_STR=${MLPGSQL_CONN_STR}
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
      - ARTIFACT_ROOT=${ARTIFACT_ROOT}
      - CONDA_DEFAULT_ENV=twit-sentiment-group-2
      - MLFLOW_TRACKING_STORE_MIGRATE=true
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow:/app
      - ./src:/app/src
      - ./.env:/app/.env
      - ./config:/app/config
    networks:
      - your_project_net
    logging:
      <<: *log-config
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5000/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    command: >
      bash -c "
        source activate twit-sentiment-group-2 &&
        echo 'Current conda environment:' &&
        echo $${CONDA_DEFAULT_ENV} &&
        python -m mlflow db upgrade $${MLPGSQL_CONN_STR} &&
        exec mlflow server --backend-store-uri=$${MLPGSQL_CONN_STR} --default-artifact-root=$${ARTIFACT_ROOT} --host 0.0.0.0 --port 5000
      "
    # depends_on: #Note cái này cho api nè
    #   - mlflow-server

networks:
  your_project_net:
    driver: bridge