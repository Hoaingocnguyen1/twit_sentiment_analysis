# -------------------------------
# Common Environment Variables for Airflow
# -------------------------------
x-airflow-common-env: &airflow-common-env
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql://hoaingocnguyen:Hn20052004!@twitwarehouse.postgres.database.azure.com:5432/postgres?sslmode=require
  AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
  AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
  AIRFLOW__CORE__PLUGINS_FOLDER: /opt/airflow/plugins
  AIRFLOW__CORE__BASE_LOG_FOLDER: /opt/airflow/logs
  AIRFLOW__WEBSERVER__SECRET_KEY: "my_ultra_secret_key_123456"
  PYTHONPATH: /opt/airflow

# -------------------------------
# Common Build Config for Airflow Services
# -------------------------------
x-airflow-common: &airflow-common
  build:
    context: ./airflow
    dockerfile: Dockerfile
  environment:
    <<: *airflow-common-env
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/plugins:/opt/airflow/plugins
    - ./src:/opt/airflow/src
    - ./config:/opt/airflow/config
    - ./airflow/requirements.txt:/requirements.txt
    - ./config/cookies.json:/opt/airflow/cookies.json
    - ./.env:/opt/airflow/.env

# -------------------------------
# Common Logging Configuration
# -------------------------------
x-log-config: &log-config
  driver: json-file
  options:
    max-size: "5m"
    max-file: "1"

# -------------------------------
# Services
# -------------------------------
services:

  airflow-webserver:
    <<: *airflow-common
    hostname: airflow-webserver
    ports:
      - "8080:8080"
    expose:
      - "8080"
    logging:
      <<: *log-config
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin_password --firstname FirstName --lastname LastName --email nn20052004@gmail.com --role Admin &&
        exec airflow webserver
      "
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    hostname: airflow-scheduler
    logging:
      <<: *log-config
    command: >
      bash -c "exec airflow scheduler"
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob"]
      interval: 10s
      timeout: 10s
      retries: 5

  fastapi:
    build:
      context: .
      dockerfile: sentimentAPI/Dockerfile.api
    container_name: fastapi_service
    ports:
      - "8000:8000"
    environment:
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
      - AZURE_BLOB_CONTAINER=${AZURE_BLOB_CONTAINER}
      - AZURE_ACCOUNT_NAME=${AZURE_ACCOUNT_NAME}
      - AZURE_SAS_TOKEN=${AZURE_SAS_TOKEN}
      - AZURE_STORAGE_ACCESS_KEY=${AZURE_STORAGE_ACCESS_KEY}
      - AZURE_STORAGE_CONNECTION_STRING=${AZURE_STORAGE_CONNECTION_STRING}
      - MODEL_NAME=${MODEL_NAME}
    volumes:
      - ./sentimentAPI:/api
      - ./src:/api/src
    networks:
      - monitoring

  prometheus:
    image: prom/prometheus
    container_name: prometheus_b
    ports:
      - "9090:9090"
    volumes:
      - ./sentimentAPI/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - monitoring

  grafana:
    image: grafana/grafana
    container_name: grafana_a
    ports:
      - "3000:3000"
    networks:
      - monitoring

# -------------------------------
# Networks and Volumes
# -------------------------------
networks:
  monitoring:
  your_project_net:
    driver: bridge

volumes:
  prometheus_data:
